{"model":{"opTime":1569234604,"supervised":1,"preprocess":0,"featureSelect":0,"classifier":1,"regressor":1,"cluster":1,"timeSeries":0,"mining":0,"pythonClass":"mine_tf_train","pythonClassImport":"import tensorflow as tf","pythonClassDefine":"class mine_tf_train:\n    def __init__(self):\n        self._params = {}\n        self.vars = {}\n        self.inference_out = None\n        self.global_step = None\n        self.lr = None\n        self.loss = None\n        self.out = None\n        self.saver = None\n\n\n    def get_params(self, deep=True):\n        return self._params\n\n    def set_params(self, **params):\n        for (k, v) in params.items():\n            self._params[k] = v\n\n    def fit(self, data, y=None):\n        return self\n\n    def transform(self, data):\n        return data\n\n    def __getstate__(self):\n        return self._params\n\n    def __setstate__(self, state):\n        self._params = state\n\n    def dump_ext_tf(self, sess, file):\n        self.saver.save(sess, file)\n\n    def load_ext_tf(self, sess, file):\n        self.saver.restore(sess, file)\n\n    def build(self, y_pred, y_true, feed_dict_train, feed_dict_test):\n        self.inference_out = y_pred\n        self.global_step = tf.Variable(0)\n        self.saver = tf.train.Saver()\n\n        loss = self._params.get('loss')\n        optimizer = self._params.get('optimizer', 'GradientDescentOptimizer')\n        learning_rate = float(self._params.get('learning_rate', 0.01))\n        lr_decay = self._params.get('lr_decay', '')\n        lr_decay_arg = self._params.get('lr_decay_arg')\n\n        if lr_decay == 'exponential_decay':\n            items = lr_decay_arg.split('_')\n            decay_steps = float(items[0])\n            decay_rate = float(items[1])\n            self.lr = tf.train.exponential_decay(self.lr, self.global_step, decay_steps, decay_rate)\n        else:\n            self.lr = tf.Variable(learning_rate)\n\n        if loss == 'cross_entropy':\n            tf.add_to_collection('losses', -tf.reduce_sum(y_true * tf.log(y_pred)))\n        self.loss = tf.add_n(tf.get_collection('losses'))\n\n        if optimizer == 'AdamOptimizer':\n            opt = tf.train.AdamOptimizer(self.lr)\n        else:  # default 'GradientDescentOptimizer':\n            opt = tf.train.GradientDescentOptimizer(self.lr)\n\n        #self.out = opt.minimize(self.loss, global_step=self.global_step)\n\n        # Compute gradients.\n        with tf.control_dependencies([self.loss]):\n            grads = opt.compute_gradients(self.loss)\n        # Apply gradients.\n        self.out = opt.apply_gradients(grads, global_step=self.global_step)","desc":"http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#optimizers","inputParams":[{"name":"batch_size","valueSet":"I>0","defaultValue":"","candidateValues":"","desc":"批次大小","option":0},{"name":"max_iteration","valueSet":"I>0","defaultValue":"","candidateValues":"","desc":"迭代次数","option":0},{"name":"loss","valueSet":"'cross_entropy'","defaultValue":"","candidateValues":"","desc":"损失","option":0},{"name":"optimizer","valueSet":"'GradientDescentOptimizer' 'AdamOptimizer'","defaultValue":"'GradientDescentOptimizer'","candidateValues":"","desc":"优化器，优化算法","option":1},{"name":"learning_rate","valueSet":"R>0","defaultValue":"0.01","candidateValues":"","desc":"学习率","option":1},{"name":"in_dim","valueSet":"str","defaultValue":"","candidateValues":"","desc":"特征值维度，初始Tensor维度。不含batch","option":0},{"name":"out_dim","valueSet":"str","defaultValue":"","candidateValues":"","desc":"目标值维度，即最终Tensor维度。不含batch","option":0},{"name":"lr_decay","valueSet":"'exponential_decay'","defaultValue":"","candidateValues":"","desc":"学习率衰减策略","option":1},{"name":"lr_decay_arg","valueSet":"str","defaultValue":"","candidateValues":"","desc":"学习率衰减策略参数","option":1}],"studyParams":[],"useExample":""}}